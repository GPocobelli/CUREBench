{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-12T14:20:25.797992Z",
     "start_time": "2026-01-12T14:20:24.903367Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from rapidfuzz import fuzz\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T14:20:29.622470Z",
     "start_time": "2026-01-12T14:20:29.618946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "METHODS = [\"agent\", \"mrr\", \"sc\", \"default\", \"cot\", \"cot2\"]\n",
    "RUN_IDS = [1, 2, 3]\n",
    "\n",
    "THRESH_LCS = 0.9\n",
    "THRESH_FUZZY = 0.95\n",
    "\n",
    "PARAM_GRID = list(itertools.product(METHODS,RUN_IDS))"
   ],
   "id": "40a0b954ed6b42df",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T14:20:47.077791Z",
     "start_time": "2026-01-12T14:20:47.055749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#define \n",
    "questions = pd.read_json('data/curebench_valset_pharse1.jsonl',lines=True) \n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def best_option_by_fuzzy(prediction: str, options_dict: dict) -> tuple[str, float]:\n",
    "    pred = _norm(prediction)\n",
    "    scores = {}\n",
    "    for k, opt in options_dict.items():\n",
    "        optn = _norm(opt)\n",
    "\n",
    "        # Primary: robust to extra words + reordering\n",
    "        s1 = fuzz.token_set_ratio(pred, optn) / 100.0\n",
    "\n",
    "        # Secondary: robust if pred includes a copied snippet of the option\n",
    "        s2 = fuzz.partial_ratio(pred, optn) / 100.0\n",
    "\n",
    "        # Blend (tweak weights if you want)\n",
    "        score = 0.7 * s1 + 0.3 * s2\n",
    "        scores[k] = score\n",
    "\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best, float(scores[best])\n",
    "\n",
    "def run(METHOD , RUN_ID, THRESH_LCS, THRESH_FUZZY ):\n",
    "    #Read data\n",
    "    path = Path(\"competition_results\",METHOD,str(RUN_ID),\"submission.csv\")\n",
    "    answers = pd.read_csv(path)\n",
    "    \n",
    "    #SPLITTING DATA INTO MC, OPEN-ENDED MC, and OPEN QUESTIONS\n",
    "    data = questions.merge(answers,on=\"id\",how=\"right\")\n",
    "    colnames = data.columns\n",
    "\n",
    "    data_mc = data[data[\"question_type\"]==\"multi_choice\"]\n",
    "    data_open_mc = data[data[\"question_type\"]==\"open_ended_multi_choice\"]\n",
    "    data_open = data[data[\"question_type\"]==\"open_ended\"]\n",
    "    \n",
    "    #####ANALYZE MC DATA #######\n",
    "    data_mc = data_mc.copy()\n",
    "    \n",
    "    for i,row in data_mc.iterrows():\n",
    "        prediction = row[\"prediction\"]\n",
    "        options = dict(row[\"options\"])\n",
    "        LCSratios = {}\n",
    "        for key in options.keys():\n",
    "            option = options[key]\n",
    "            LCSratios[key] = SequenceMatcher(None, prediction, option).ratio()\n",
    "        \n",
    "        best_pred = max(LCSratios, key=LCSratios.get)\n",
    "        data_mc.loc[i,\"pred_LCS\"] = best_pred\n",
    "        data_mc.loc[i,\"LCS_similarity\"]=LCSratios[best_pred]\n",
    "    \n",
    "    data_mc['final_choice'] = np.where(\n",
    "        data_mc['LCS_similarity'] > THRESH_LCS,\n",
    "        data_mc['pred_LCS'],\n",
    "        data_mc['choice']    \n",
    "    )\n",
    "    \n",
    "    data_mc[\"is_correct\"] = data_mc['final_choice'] == data_mc[\"correct_answer\"]     \n",
    "    NaN_mc = len(data_mc[data_mc[\"final_choice\"] == 'NOTAVALUE'])\n",
    "    valid_mc = len(data_mc) - NaN_mc\n",
    "    ACC_mc = data_mc[\"is_correct\"].sum() / valid_mc\n",
    "    \n",
    "    #ANALYZE OPEN MC DATA\n",
    "    data_open_mc = data_open_mc.copy()\n",
    "\n",
    "    for i,row in data_open_mc.iterrows():\n",
    "        prediction = row[\"prediction\"]\n",
    "        options = dict(row[\"options\"])\n",
    "        LCSratios = {}\n",
    "        for key in options.keys():\n",
    "            option = options[key]\n",
    "            LCSratios[key] = SequenceMatcher(None, prediction, option).ratio()\n",
    "        \n",
    "        best_pred = max(LCSratios, key=LCSratios.get)\n",
    "        data_open_mc.loc[i,\"pred_LCS\"] = best_pred\n",
    "        data_open_mc.loc[i,\"LCS_similarity\"] = LCSratios[best_pred]\n",
    "        \n",
    "    data_open_mc['final_choice'] = np.where(\n",
    "        data_open_mc['LCS_similarity'] > THRESH_LCS,\n",
    "        data_open_mc['pred_LCS'],\n",
    "        data_open_mc['choice']\n",
    "    )\n",
    "    \n",
    "    data_open_mc['is_correct'] = data_open_mc[\"final_choice\"] == data_open_mc[\"correct_answer\"]    \n",
    "    NaN_open_mc = len(data_open_mc[data_open_mc[\"final_choice\"] == 'NOTAVALUE'])\n",
    "    valid_open_mc = len(data_open_mc) - NaN_open_mc\n",
    "    ACC_open_mc = data_open_mc[\"is_correct\"].sum() / valid_open_mc\n",
    "\n",
    "    #TOTAL ACCURACY (excl. open questions)\n",
    "    ACC_total = ((ACC_mc*valid_mc + \n",
    "             ACC_open_mc*valid_open_mc) / \n",
    "             (valid_open_mc + valid_mc))\n",
    "\n",
    "    #ANALYZE OPEN QUESTIONS\n",
    "    data_open = data_open.copy()\n",
    "    \n",
    "    fuzz_results = data_open.apply(\n",
    "        lambda r: best_option_by_fuzzy(r[\"prediction\"], dict(r[\"options\"])),\n",
    "        axis=1\n",
    "    )\n",
    "    data_open[\"pred_LCS\"] = fuzz_results.map(lambda x: x[0])\n",
    "    data_open[\"LCS_similarity\"] = fuzz_results.map(lambda x: x[1])\n",
    "    \n",
    "    \n",
    "    data_open[\"final_choice\"] = np.where(\n",
    "        data_open[\"LCS_similarity\"] >= THRESH_FUZZY,\n",
    "        data_open[\"pred_LCS\"],\n",
    "        data_open[\"choice\"])\n",
    "    \n",
    "    data_open[\"is_correct\"] = data_open[\"final_choice\"] == data_open[\"correct_answer\"]\n",
    "    \n",
    "    NaN_open = len(data_open[data_open[\"final_choice\"] == 'NOTAVALUE'])\n",
    "    valid_open = len(data_open) - NaN_open\n",
    "    ACC_open = data_open[\"is_correct\"].sum() / valid_open\n",
    "    \n",
    "    #COMBINE DATA INTO DATAFRAMES:\n",
    "    # det: detailed predictions, \n",
    "    # acc: summuary statistics\n",
    "    det = pd.concat(\n",
    "        [data_mc,\n",
    "        data_open_mc,\n",
    "        data_open],\n",
    "        axis=0\n",
    "    )\n",
    "    det['method']=METHOD\n",
    "    det['run_id']=RUN_ID\n",
    "    det.rename(columns={'pred_LCS':'pred_SIM',\n",
    "                        'LCS_similarity':'SIM_score',\n",
    "                        'id':'question_id'},\n",
    "               inplace=True)\n",
    "    det = det[\n",
    "        ['method','run_id','question_type','question_id','question',\n",
    "         'correct_answer','options','prediction','choice','reasoning','pred_SIM',\n",
    "         'SIM_score','final_choice','is_correct']\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    acc = pd.DataFrame(\n",
    "        {'method':[METHOD]*4,\n",
    "         'run_id':[RUN_ID]*4,\n",
    "         'question_type':['multi_choice','open_ended_multi_choice','open_ended','total'],\n",
    "         })\n",
    "    acc['accuracy'] = [ACC_mc,ACC_open_mc,ACC_open,ACC_total]\n",
    "    acc['valid_answers'] = [valid_mc,valid_open_mc,valid_open,valid_mc+valid_open_mc]\n",
    "    acc['invalid_answers'] = [NaN_mc,NaN_open_mc,NaN_open,NaN_mc+NaN_open_mc]\n",
    "    acc['prop_valid'] = acc['valid_answers'] / (acc['valid_answers']+acc['invalid_answers'])\n",
    "    acc['similarity_threshold'] = [THRESH_LCS,THRESH_LCS,THRESH_FUZZY,THRESH_LCS]\n",
    "    \n",
    "    return acc, det\n",
    "    \n"
   ],
   "id": "b55242424e42e40b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T14:21:05.974914Z",
     "start_time": "2026-01-12T14:20:49.618964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### gogo sanchez ski shoes\n",
    "ACC = pd.DataFrame(\n",
    "    columns=['method','run_id','question_type','accuracy','valid_answers','invalid_answers','prop_valid','similarity_threshold']\n",
    ")\n",
    "DET = pd.DataFrame(\n",
    "    columns = ['method','run_id','question_type','question_id','question',\n",
    "         'correct_answer','options','prediction','choice','reasoning','pred_SIM',\n",
    "         'SIM_score','final_choice','is_correct']\n",
    ")\n",
    "\n",
    "for METHOD, RUN_ID in PARAM_GRID:\n",
    "    acc,det = run(METHOD=METHOD,\n",
    "        RUN_ID=RUN_ID,\n",
    "        THRESH_LCS=THRESH_LCS,\n",
    "        THRESH_FUZZY=THRESH_FUZZY)\n",
    "    \n",
    "    ACC = pd.concat([ACC,acc],axis=0)\n",
    "    DET = pd.concat([DET,det],axis=0)\n",
    "    "
   ],
   "id": "e22f30ffb6f608f4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/835ddmgd5_dbbt66qrt26vlh0000gn/T/ipykernel_28646/1839220097.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ACC = pd.concat([ACC,acc],axis=0)\n",
      "/var/folders/18/835ddmgd5_dbbt66qrt26vlh0000gn/T/ipykernel_28646/1839220097.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  DET = pd.concat([DET,det],axis=0)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T14:31:09.870127Z",
     "start_time": "2026-01-12T14:31:09.865731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ACC_f = ACC[ACC['question_type']!='open_ended']  \n",
    "ACC_f = ACC_f.sort_values(by=['question_type','prop_valid'],ascending=False)\n"
   ],
   "id": "ee7df7794d088ccd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T14:33:56.390839Z",
     "start_time": "2026-01-12T14:33:56.384390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Save results\n",
    "ACC.to_csv('accuracy_summary.csv',index=False)\n",
    "ACC_f.to_csv('accuracy_summary_excl_open.csv',index=False)"
   ],
   "id": "cffcd74d99a5be5b",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
